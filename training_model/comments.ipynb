{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments Dataset Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:41:40.803107Z",
     "iopub.status.busy": "2025-09-09T14:41:40.802685Z",
     "iopub.status.idle": "2025-09-09T14:41:41.056537Z",
     "shell.execute_reply": "2025-09-09T14:41:41.055985Z",
     "shell.execute_reply.started": "2025-09-09T14:41:40.803085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Comment.csv\n",
    "- importing and merging all comemnts.csv\n",
    "- randomizing it\n",
    "- taking only 10% = 50,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:41:41.536413Z",
     "iopub.status.busy": "2025-09-09T14:41:41.535739Z",
     "iopub.status.idle": "2025-09-09T14:42:06.401258Z",
     "shell.execute_reply": "2025-09-09T14:42:06.400641Z",
     "shell.execute_reply.started": "2025-09-09T14:41:41.536393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path = '../data/comments*.csv'\n",
    "cmtFiles = glob.glob(path)\n",
    "cmtFilesList = []\n",
    "\n",
    "for filename in cmtFiles:\n",
    "    comments_df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    cmtFilesList.append(comments_df)\n",
    "\n",
    "comments_df = pd.concat(cmtFilesList, axis=0, ignore_index=True)\n",
    "\n",
    "comments_df = comments_df.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:42:06.402547Z",
     "iopub.status.busy": "2025-09-09T14:42:06.402303Z",
     "iopub.status.idle": "2025-09-09T14:42:07.819332Z",
     "shell.execute_reply": "2025-09-09T14:42:07.818544Z",
     "shell.execute_reply.started": "2025-09-09T14:42:06.402522Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "videos_df = pd.read_csv('../data/cleaned_videos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:42:07.820244Z",
     "iopub.status.busy": "2025-09-09T14:42:07.820033Z",
     "iopub.status.idle": "2025-09-09T14:42:08.087096Z",
     "shell.execute_reply": "2025-09-09T14:42:08.086491Z",
     "shell.execute_reply.started": "2025-09-09T14:42:07.820228Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 472501 entries, 3956772 to 4309356\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   kind             472501 non-null  object \n",
      " 1   commentId        472501 non-null  int64  \n",
      " 2   channelId        472501 non-null  int64  \n",
      " 3   videoId          472501 non-null  int64  \n",
      " 4   authorId         472501 non-null  int64  \n",
      " 5   textOriginal     472483 non-null  object \n",
      " 6   parentCommentId  51928 non-null   float64\n",
      " 7   likeCount        472501 non-null  int64  \n",
      " 8   publishedAt      472501 non-null  object \n",
      " 9   updatedAt        472501 non-null  object \n",
      "dtypes: float64(1), int64(5), object(4)\n",
      "memory usage: 39.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(comments_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "- drop irrelevant columns\n",
    "- removing null comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:42:08.088851Z",
     "iopub.status.busy": "2025-09-09T14:42:08.088658Z",
     "iopub.status.idle": "2025-09-09T14:42:08.407713Z",
     "shell.execute_reply": "2025-09-09T14:42:08.407014Z",
     "shell.execute_reply.started": "2025-09-09T14:42:08.088836Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 472483 entries, 3956772 to 4309356\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   commentId        472483 non-null  int64  \n",
      " 1   channelId        472483 non-null  int64  \n",
      " 2   videoId          472483 non-null  int64  \n",
      " 3   authorId         472483 non-null  int64  \n",
      " 4   textOriginal     472483 non-null  object \n",
      " 5   parentCommentId  51928 non-null   float64\n",
      " 6   likeCount        472483 non-null  int64  \n",
      "dtypes: float64(1), int64(5), object(1)\n",
      "memory usage: 28.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "comments_df = comments_df.drop(columns=['kind','publishedAt','updatedAt'])\n",
    "\n",
    "comments_df = comments_df.dropna(subset=['textOriginal'])\n",
    "\n",
    "print(comments_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoji to Text Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:42:08.408718Z",
     "iopub.status.busy": "2025-09-09T14:42:08.408469Z",
     "iopub.status.idle": "2025-09-09T14:42:12.427152Z",
     "shell.execute_reply": "2025-09-09T14:42:12.426264Z",
     "shell.execute_reply.started": "2025-09-09T14:42:08.408699Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting demoji\n",
      "  Downloading demoji-1.1.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: demoji\n",
      "Successfully installed demoji-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:42:12.428705Z",
     "iopub.status.busy": "2025-09-09T14:42:12.428377Z",
     "iopub.status.idle": "2025-09-09T14:44:55.046823Z",
     "shell.execute_reply": "2025-09-09T14:44:55.046220Z",
     "shell.execute_reply.started": "2025-09-09T14:42:12.428672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import demoji\n",
    "\n",
    "def emojitoText (text):\n",
    "    return demoji.replace_with_desc(text, sep=' ')\n",
    "\n",
    "comments_df['textCleaned'] = comments_df['textOriginal'].apply(emojitoText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise Fancy Font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "def normaliseFont (text):\n",
    "    return unidecode(text)\n",
    "\n",
    "comments_df['textCleaned'] = comments_df['textOriginal'].apply(normaliseFont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Removal\n",
    "- website link\n",
    "- user ID\n",
    "- long numbers (bank account / phone numbers)\n",
    "- punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:48:07.429612Z",
     "iopub.status.busy": "2025-09-09T14:48:07.428931Z",
     "iopub.status.idle": "2025-09-09T14:48:10.693917Z",
     "shell.execute_reply": "2025-09-09T14:48:10.693390Z",
     "shell.execute_reply.started": "2025-09-09T14:48:07.429587Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "urlPattern = r'\\b(?:(?:https?:\\/\\/|www\\.)[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?(?:\\.[a-z]{2,})+(?::\\d{2,5})?(?:\\/[^\\s]*)?)\\b'\n",
    "comments_df['textCleaned'] = comments_df['textCleaned'].str.replace(urlPattern, '', regex=True)\n",
    "\n",
    "userPattern = r'@\\w+'\n",
    "comments_df['textCleaned'] = comments_df['textCleaned'].str.replace(userPattern, '', regex=True)\n",
    "\n",
    "numberPattern = r'\\b\\d{10,}\\b'\n",
    "comments_df['textCleaned'] = comments_df['textCleaned'].str.replace(numberPattern, '', regex=True)\n",
    "\n",
    "punctuationPattern = r'[^\\w\\s]'\n",
    "comments_df['textCleaned'] = comments_df['textCleaned'].str.replace(punctuationPattern, '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:48:28.016608Z",
     "iopub.status.busy": "2025-09-09T14:48:28.015911Z",
     "iopub.status.idle": "2025-09-09T14:48:30.422570Z",
     "shell.execute_reply": "2025-09-09T14:48:30.421788Z",
     "shell.execute_reply.started": "2025-09-09T14:48:28.016583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "comments_df['textCleaned'] = comments_df['textCleaned'].str.lower()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def removeStopwords(text):\n",
    "    words = text.split()\n",
    "    filteredWords = [word for word in words if word not in stopWords]\n",
    "    return ' '.join(filteredWords)\n",
    "\n",
    "comments_df['textCleaned'] = comments_df['textCleaned'].apply(removeStopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:48:31.245812Z",
     "iopub.status.busy": "2025-09-09T14:48:31.245467Z",
     "iopub.status.idle": "2025-09-09T14:48:31.450282Z",
     "shell.execute_reply": "2025-09-09T14:48:31.449686Z",
     "shell.execute_reply.started": "2025-09-09T14:48:31.245794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "comments_df = comments_df.dropna(subset=['textCleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment Dataset Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection\n",
    "- remove non english comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:48:38.546629Z",
     "iopub.status.busy": "2025-09-09T14:48:38.546033Z",
     "iopub.status.idle": "2025-09-09T14:48:43.928340Z",
     "shell.execute_reply": "2025-09-09T14:48:43.927526Z",
     "shell.execute_reply.started": "2025-09-09T14:48:38.546609Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=c7ae176c8a4408328bf571a50f1648b4025bad9e31b9de655357ffef8ba357b2\n",
      "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:48:49.622279Z",
     "iopub.status.busy": "2025-09-09T14:48:49.622022Z",
     "iopub.status.idle": "2025-09-09T15:16:21.810490Z",
     "shell.execute_reply": "2025-09-09T15:16:21.809665Z",
     "shell.execute_reply.started": "2025-09-09T14:48:49.622258Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of comments: 472483\n",
      "Number of English comments found: 246486\n",
      "Number of non-English comments dropped: 225997\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "initial_count = len(comments_df)\n",
    "print(f\"Initial number of comments: {initial_count}\")\n",
    "\n",
    "english_mask = comments_df['textOriginal'].apply(is_english)\n",
    "comments_df = comments_df[english_mask]\n",
    "\n",
    "final_count = len(comments_df)\n",
    "print(f\"Number of English comments found: {final_count}\")\n",
    "print(f\"Number of non-English comments dropped: {initial_count - final_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Comments & Videos CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T15:17:33.938824Z",
     "iopub.status.busy": "2025-09-09T15:17:33.938544Z",
     "iopub.status.idle": "2025-09-09T15:17:34.932521Z",
     "shell.execute_reply": "2025-09-09T15:17:34.931747Z",
     "shell.execute_reply.started": "2025-09-09T15:17:33.938805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_merged = pd.merge(comments_df, videos_df, on='videoId', how='left')\n",
    "\n",
    "df_merged['title'] = df_merged['title'].fillna('')\n",
    "df_merged['description'] = df_merged['description'].fillna('')\n",
    "df_merged['tags'] = df_merged['tags'].fillna('')\n",
    "df_merged['textCleaned'] = df_merged['textCleaned'].fillna('')\n",
    "\n",
    "df_merged['video_text'] = df_merged['title'] + ' ' + df_merged['description'] + ' ' + df_merged['tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevancy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T15:18:42.419903Z",
     "iopub.status.busy": "2025-09-09T15:18:42.419620Z",
     "iopub.status.idle": "2025-09-09T15:23:49.842399Z",
     "shell.execute_reply": "2025-09-09T15:23:49.841735Z",
     "shell.execute_reply.started": "2025-09-09T15:18:42.419880Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e651ac5da8114162a48518a0e2d8210e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7703 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdab05f0c6a4dcaa6c351498f92d31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7703 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "video_embeddings = model.encode(df_merged['video_text'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "comment_embeddings = model.encode(df_merged['textCleaned'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T15:24:30.378681Z",
     "iopub.status.busy": "2025-09-09T15:24:30.378362Z",
     "iopub.status.idle": "2025-09-09T15:24:30.555298Z",
     "shell.execute_reply": "2025-09-09T15:24:30.554704Z",
     "shell.execute_reply.started": "2025-09-09T15:24:30.378658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 256\n",
    "all_relevancy_scores = []\n",
    "\n",
    "for i in range(0, len(comment_embeddings), batch_size):\n",
    "    comment_batch = comment_embeddings[i:i + batch_size]\n",
    "    video_batch = video_embeddings[i:i + batch_size] \n",
    "    cosine_scores_batch = util.cos_sim(comment_batch, video_batch)\n",
    "    relevancy_scores_batch = cosine_scores_batch.diag()\n",
    "    all_relevancy_scores.append(relevancy_scores_batch)\n",
    "\n",
    "relevancyScore = torch.cat(all_relevancy_scores)\n",
    "df_merged.loc[df_merged.index, 'relevancyScore'] = relevancyScore.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T15:24:32.480484Z",
     "iopub.status.busy": "2025-09-09T15:24:32.480219Z",
     "iopub.status.idle": "2025-09-09T15:24:33.825905Z",
     "shell.execute_reply": "2025-09-09T15:24:33.825131Z",
     "shell.execute_reply.started": "2025-09-09T15:24:32.480463Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-spam comments: 136377\n",
      "Number of spam comments:     110109\n"
     ]
    }
   ],
   "source": [
    "# Advertisement\n",
    "# Website link count\n",
    "df_merged['adCount'] = df_merged['textOriginal'].str.count(urlPattern)\n",
    "\n",
    "# Phone or Bank account count\n",
    "df_merged['adCount'] += df_merged['textOriginal'].str.count(numberPattern)\n",
    "\n",
    "# Repetition (same author id AND same text) OR (same text)\n",
    "df_merged['isRepetition'] = df_merged.duplicated(subset=['authorId', 'textOriginal'], keep='first') | df_merged.duplicated(subset=['textOriginal'], keep='first')\n",
    "\n",
    "# Relevancy Score \n",
    "relevancy_threshold = 0.2\n",
    "\n",
    "# Spam Detection\n",
    "df_merged['spam'] = ((df_merged['adCount'] > 0) | (df_merged['isRepetition'] == True)).astype(int) | (df_merged['relevancyScore'] <= relevancy_threshold)\n",
    "\n",
    "spam_counts = df_merged['spam'].value_counts()\n",
    "print(f\"Number of non-spam comments: {spam_counts.get(False, 0)}\")\n",
    "print(f\"Number of spam comments:     {spam_counts.get(True, 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T15:27:41.624765Z",
     "iopub.status.busy": "2025-09-09T15:27:41.624150Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Assume 'df_merged' is your DataFrame with 'spam' and 'textCleaned' columns\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "comment_categories = ['Product Feedback', 'Brand Sentiment', 'Customer Inquiry', 'User Engagement']\n",
    "\n",
    "# Create a clean copy to avoid warnings\n",
    "df_non_spam = df_merged[df_merged['spam'] == False].copy()\n",
    "\n",
    "# Create a mask to find comments that have actual text\n",
    "valid_text_mask = df_non_spam['textCleaned'].str.strip() != ''\n",
    "\n",
    "# Select only the non-empty comments to classify\n",
    "comments_to_classify = df_non_spam.loc[valid_text_mask, 'textCleaned'].tolist()\n",
    "\n",
    "# Initialize the new column with a default placeholder\n",
    "df_non_spam['commentCategory'] = 'Uncategorized'\n",
    "\n",
    "# Run the classifier only on the valid comments\n",
    "if comments_to_classify:\n",
    "    results = classifier(comments_to_classify, comment_categories, batch_size=32)\n",
    "    classified_categories = [result['labels'][0] for result in results]\n",
    "    \n",
    "    # Place results back into the correct rows using the mask\n",
    "    df_non_spam.loc[valid_text_mask, 'commentCategory'] = classified_categories\n",
    "\n",
    "# Display a preview of the results\n",
    "print(df_non_spam[['spam', 'textCleaned', 'commentCategory']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T13:38:34.819760Z",
     "iopub.status.busy": "2025-09-09T13:38:34.819111Z",
     "iopub.status.idle": "2025-09-09T13:49:14.874056Z",
     "shell.execute_reply": "2025-09-09T13:49:14.873131Z",
     "shell.execute_reply.started": "2025-09-09T13:38:34.819736Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6226fd09b50b4f13a97290ee429c9356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8d2da0efe84dee8f963a7f47a18f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c6e99a5e74405aa0ff494b33cb57e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649d82eb43554831badb6d5333add85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment for 138956 comments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/664397523.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_non_spam['sentiment_label'] = [result['label'] for result in results]\n",
      "/tmp/ipykernel_36/664397523.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_non_spam['sentiment_score'] = [result['score'] for result in results]\n"
     ]
    }
   ],
   "source": [
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "comments_to_analyze = df_non_spam['textCleaned'].tolist()\n",
    "\n",
    "if comments_to_analyze:\n",
    "    print(f\"Analyzing sentiment for {len(comments_to_analyze)} comments...\")\n",
    "    results = sentiment_pipe(comments_to_analyze, batch_size=64, truncation=True)\n",
    "\n",
    "    df_non_spam['sentiment_label'] = [result['label'] for result in results]\n",
    "    df_non_spam['sentiment_score'] = [result['score'] for result in results]\n",
    "\n",
    "else:\n",
    "    print(\"No comments to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_non_spam.to_csv('final_comments.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8195445,
     "sourceId": 12950141,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8217452,
     "sourceId": 12982979,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
