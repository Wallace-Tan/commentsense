{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12950141,"sourceType":"datasetVersion","datasetId":8195445},{"sourceId":12982979,"sourceType":"datasetVersion","datasetId":8217452}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Comments Dataset Cleaning ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport glob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:41:40.802685Z","iopub.execute_input":"2025-09-09T14:41:40.803107Z","iopub.status.idle":"2025-09-09T14:41:41.056537Z","shell.execute_reply.started":"2025-09-09T14:41:40.803085Z","shell.execute_reply":"2025-09-09T14:41:41.055985Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Combine Comment.csv\n- importing and merging all comemnts.csv\n- randomizing it\n- taking only 10% = 50,000","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/loreal-datathon/comments*.csv'\ncmtFiles = glob.glob(path)\ncmtFilesList = []\n\nfor filename in cmtFiles:\n    comments_df = pd.read_csv(filename, index_col=None, header=0)\n    cmtFilesList.append(comments_df)\n\ncomments_df = pd.concat(cmtFilesList, axis=0, ignore_index=True)\n\ncomments_df = comments_df.sample(frac=0.1, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:41:41.535739Z","iopub.execute_input":"2025-09-09T14:41:41.536413Z","iopub.status.idle":"2025-09-09T14:42:06.401258Z","shell.execute_reply.started":"2025-09-09T14:41:41.536393Z","shell.execute_reply":"2025-09-09T14:42:06.400641Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"videos_df = pd.read_csv('/kaggle/input/loreal-datathon/videos.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:42:06.402303Z","iopub.execute_input":"2025-09-09T14:42:06.402547Z","iopub.status.idle":"2025-09-09T14:42:07.819332Z","shell.execute_reply.started":"2025-09-09T14:42:06.402522Z","shell.execute_reply":"2025-09-09T14:42:07.818544Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"print(comments_df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:42:07.820033Z","iopub.execute_input":"2025-09-09T14:42:07.820244Z","iopub.status.idle":"2025-09-09T14:42:08.087096Z","shell.execute_reply.started":"2025-09-09T14:42:07.820228Z","shell.execute_reply":"2025-09-09T14:42:08.086491Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 472501 entries, 3956772 to 4309356\nData columns (total 10 columns):\n #   Column           Non-Null Count   Dtype  \n---  ------           --------------   -----  \n 0   kind             472501 non-null  object \n 1   commentId        472501 non-null  int64  \n 2   channelId        472501 non-null  int64  \n 3   videoId          472501 non-null  int64  \n 4   authorId         472501 non-null  int64  \n 5   textOriginal     472483 non-null  object \n 6   parentCommentId  51928 non-null   float64\n 7   likeCount        472501 non-null  int64  \n 8   publishedAt      472501 non-null  object \n 9   updatedAt        472501 non-null  object \ndtypes: float64(1), int64(5), object(4)\nmemory usage: 39.7+ MB\nNone\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Data Preprocessing\n- drop irrelevant columns\n- removing null comments ","metadata":{}},{"cell_type":"code","source":"comments_df = comments_df.drop(columns=['kind','publishedAt','updatedAt'])\n\ncomments_df = comments_df.dropna(subset=['textOriginal'])\n\nprint(comments_df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:42:08.088658Z","iopub.execute_input":"2025-09-09T14:42:08.088851Z","iopub.status.idle":"2025-09-09T14:42:08.407713Z","shell.execute_reply.started":"2025-09-09T14:42:08.088836Z","shell.execute_reply":"2025-09-09T14:42:08.407014Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 472483 entries, 3956772 to 4309356\nData columns (total 7 columns):\n #   Column           Non-Null Count   Dtype  \n---  ------           --------------   -----  \n 0   commentId        472483 non-null  int64  \n 1   channelId        472483 non-null  int64  \n 2   videoId          472483 non-null  int64  \n 3   authorId         472483 non-null  int64  \n 4   textOriginal     472483 non-null  object \n 5   parentCommentId  51928 non-null   float64\n 6   likeCount        472483 non-null  int64  \ndtypes: float64(1), int64(5), object(1)\nmemory usage: 28.8+ MB\nNone\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Emoji to Text Conversion","metadata":{}},{"cell_type":"code","source":"pip install demoji","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:42:08.408469Z","iopub.execute_input":"2025-09-09T14:42:08.408718Z","iopub.status.idle":"2025-09-09T14:42:12.427152Z","shell.execute_reply.started":"2025-09-09T14:42:08.408699Z","shell.execute_reply":"2025-09-09T14:42:12.426264Z"}},"outputs":[{"name":"stdout","text":"Collecting demoji\n  Downloading demoji-1.1.0-py3-none-any.whl.metadata (9.2 kB)\nDownloading demoji-1.1.0-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: demoji\nSuccessfully installed demoji-1.1.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import demoji\n\ndef emojitoText (text):\n    return demoji.replace_with_desc(text, sep=' ')\n\ncomments_df['textCleaned'] = comments_df['textOriginal'].apply(emojitoText)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:42:12.428377Z","iopub.execute_input":"2025-09-09T14:42:12.428705Z","iopub.status.idle":"2025-09-09T14:44:55.046823Z","shell.execute_reply.started":"2025-09-09T14:42:12.428672Z","shell.execute_reply":"2025-09-09T14:44:55.046220Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Normalise Fancy Font","metadata":{}},{"cell_type":"code","source":"from unidecode import unidecode\n\ndef normaliseFont (text):\n    return unidecode(text)\n\ncomments_df['textCleaned'] = comments_df['textOriginal'].apply(normaliseFont)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Removal\n- website link\n- user ID\n- long numbers (bank account / phone numbers)\n- punctuation ","metadata":{}},{"cell_type":"code","source":"import re\n\nurlPattern = r'\\b(?:(?:https?:\\/\\/|www\\.)[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?(?:\\.[a-z]{2,})+(?::\\d{2,5})?(?:\\/[^\\s]*)?)\\b'\ncomments_df['textCleaned'] = comments_df['textCleaned'].str.replace(urlPattern, '', regex=True)\n\nuserPattern = r'@\\w+'\ncomments_df['textCleaned'] = comments_df['textCleaned'].str.replace(userPattern, '', regex=True)\n\nnumberPattern = r'\\b\\d{10,}\\b'\ncomments_df['textCleaned'] = comments_df['textCleaned'].str.replace(numberPattern, '', regex=True)\n\npunctuationPattern = r'[^\\w\\s]'\ncomments_df['textCleaned'] = comments_df['textCleaned'].str.replace(punctuationPattern, '', regex=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:48:07.428931Z","iopub.execute_input":"2025-09-09T14:48:07.429612Z","iopub.status.idle":"2025-09-09T14:48:10.693917Z","shell.execute_reply.started":"2025-09-09T14:48:07.429587Z","shell.execute_reply":"2025-09-09T14:48:10.693390Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Stop Word Removal","metadata":{}},{"cell_type":"code","source":"comments_df['textCleaned'] = comments_df['textCleaned'].str.lower()\n\nfrom nltk.corpus import stopwords\nstopWords = set(stopwords.words('english'))\n\ndef removeStopwords(text):\n    words = text.split()\n    filteredWords = [word for word in words if word not in stopWords]\n    return ' '.join(filteredWords)\n\ncomments_df['textCleaned'] = comments_df['textCleaned'].apply(removeStopwords)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:48:28.015911Z","iopub.execute_input":"2025-09-09T14:48:28.016608Z","iopub.status.idle":"2025-09-09T14:48:30.422570Z","shell.execute_reply.started":"2025-09-09T14:48:28.016583Z","shell.execute_reply":"2025-09-09T14:48:30.421788Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"comments_df = comments_df.dropna(subset=['textCleaned'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:48:31.245467Z","iopub.execute_input":"2025-09-09T14:48:31.245812Z","iopub.status.idle":"2025-09-09T14:48:31.450282Z","shell.execute_reply.started":"2025-09-09T14:48:31.245794Z","shell.execute_reply":"2025-09-09T14:48:31.449686Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Comment Dataset Model Training","metadata":{}},{"cell_type":"markdown","source":"## Language Detection\n- remove non english comments","metadata":{}},{"cell_type":"code","source":"pip install langdetect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:48:38.546033Z","iopub.execute_input":"2025-09-09T14:48:38.546629Z","iopub.status.idle":"2025-09-09T14:48:43.928340Z","shell.execute_reply.started":"2025-09-09T14:48:38.546609Z","shell.execute_reply":"2025-09-09T14:48:43.927526Z"}},"outputs":[{"name":"stdout","text":"Collecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\nBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=c7ae176c8a4408328bf571a50f1648b4025bad9e31b9de655357ffef8ba357b2\n  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\nSuccessfully built langdetect\nInstalling collected packages: langdetect\nSuccessfully installed langdetect-1.0.9\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from langdetect import detect, LangDetectException\n\ndef is_english(text):\n    try:\n        return detect(text) == 'en'\n    except LangDetectException:\n        return False\n\ninitial_count = len(comments_df)\nprint(f\"Initial number of comments: {initial_count}\")\n\nenglish_mask = comments_df['textOriginal'].apply(is_english)\ncomments_df = comments_df[english_mask]\n\nfinal_count = len(comments_df)\nprint(f\"Number of English comments found: {final_count}\")\nprint(f\"Number of non-English comments dropped: {initial_count - final_count}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T14:48:49.622022Z","iopub.execute_input":"2025-09-09T14:48:49.622279Z","iopub.status.idle":"2025-09-09T15:16:21.810490Z","shell.execute_reply.started":"2025-09-09T14:48:49.622258Z","shell.execute_reply":"2025-09-09T15:16:21.809665Z"}},"outputs":[{"name":"stdout","text":"Initial number of comments: 472483\nNumber of English comments found: 246486\nNumber of non-English comments dropped: 225997\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### Merge Comments & Videos CSV","metadata":{}},{"cell_type":"code","source":"df_merged = pd.merge(comments_df, videos_df, on='videoId', how='left')\n\ndf_merged['title'] = df_merged['title'].fillna('')\ndf_merged['description'] = df_merged['description'].fillna('')\ndf_merged['tags'] = df_merged['tags'].fillna('')\ndf_merged['textCleaned'] = df_merged['textCleaned'].fillna('')\n\ndf_merged['video_text'] = df_merged['title'] + ' ' + df_merged['description'] + ' ' + df_merged['tags']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:17:33.938544Z","iopub.execute_input":"2025-09-09T15:17:33.938824Z","iopub.status.idle":"2025-09-09T15:17:34.932521Z","shell.execute_reply.started":"2025-09-09T15:17:33.938805Z","shell.execute_reply":"2025-09-09T15:17:34.931747Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Relevancy Score","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nvideo_embeddings = model.encode(df_merged['video_text'].tolist(), convert_to_tensor=True, show_progress_bar=True)\ncomment_embeddings = model.encode(df_merged['textCleaned'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:18:42.419620Z","iopub.execute_input":"2025-09-09T15:18:42.419903Z","iopub.status.idle":"2025-09-09T15:23:49.842399Z","shell.execute_reply.started":"2025-09-09T15:18:42.419880Z","shell.execute_reply":"2025-09-09T15:23:49.841735Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/7703 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e651ac5da8114162a48518a0e2d8210e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/7703 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fdab05f0c6a4dcaa6c351498f92d31b"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import torch\n\nbatch_size = 256\nall_relevancy_scores = []\n\nfor i in range(0, len(comment_embeddings), batch_size):\n    comment_batch = comment_embeddings[i:i + batch_size]\n    video_batch = video_embeddings[i:i + batch_size] \n    cosine_scores_batch = util.cos_sim(comment_batch, video_batch)\n    relevancy_scores_batch = cosine_scores_batch.diag()\n    all_relevancy_scores.append(relevancy_scores_batch)\n\nrelevancyScore = torch.cat(all_relevancy_scores)\ndf_merged.loc[df_merged.index, 'relevancyScore'] = relevancyScore.cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:24:30.378362Z","iopub.execute_input":"2025-09-09T15:24:30.378681Z","iopub.status.idle":"2025-09-09T15:24:30.555298Z","shell.execute_reply.started":"2025-09-09T15:24:30.378658Z","shell.execute_reply":"2025-09-09T15:24:30.554704Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Spam Detection","metadata":{}},{"cell_type":"code","source":"# Advertisement\n# Website link count\ndf_merged['adCount'] = df_merged['textOriginal'].str.count(urlPattern)\n\n# Phone or Bank account count\ndf_merged['adCount'] += df_merged['textOriginal'].str.count(numberPattern)\n\n# Repetition (same author id AND same text) OR (same text)\ndf_merged['isRepetition'] = df_merged.duplicated(subset=['authorId', 'textOriginal'], keep='first') | df_merged.duplicated(subset=['textOriginal'], keep='first')\n\n# Relevancy Score \nrelevancy_threshold = 0.2\n\n# Spam Detection\ndf_merged['spam'] = ((df_merged['adCount'] > 0) | (df_merged['isRepetition'] == True)).astype(int) | (df_merged['relevancyScore'] <= relevancy_threshold)\n\nspam_counts = df_merged['spam'].value_counts()\nprint(f\"Number of non-spam comments: {spam_counts.get(False, 0)}\")\nprint(f\"Number of spam comments:     {spam_counts.get(True, 0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:24:32.480219Z","iopub.execute_input":"2025-09-09T15:24:32.480484Z","iopub.status.idle":"2025-09-09T15:24:33.825905Z","shell.execute_reply.started":"2025-09-09T15:24:32.480463Z","shell.execute_reply":"2025-09-09T15:24:33.825131Z"}},"outputs":[{"name":"stdout","text":"Number of non-spam comments: 136377\nNumber of spam comments:     110109\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Categorisation","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nimport pandas as pd\n\n# Assume 'df_merged' is your DataFrame with 'spam' and 'textCleaned' columns\n\nclassifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\ncomment_categories = ['Product Feedback', 'Brand Sentiment', 'Customer Inquiry', 'User Engagement']\n\n# Create a clean copy to avoid warnings\ndf_non_spam = df_merged[df_merged['spam'] == False].copy()\n\n# Create a mask to find comments that have actual text\nvalid_text_mask = df_non_spam['textCleaned'].str.strip() != ''\n\n# Select only the non-empty comments to classify\ncomments_to_classify = df_non_spam.loc[valid_text_mask, 'textCleaned'].tolist()\n\n# Initialize the new column with a default placeholder\ndf_non_spam['commentCategory'] = 'Uncategorized'\n\n# Run the classifier only on the valid comments\nif comments_to_classify:\n    results = classifier(comments_to_classify, comment_categories, batch_size=32)\n    classified_categories = [result['labels'][0] for result in results]\n    \n    # Place results back into the correct rows using the mask\n    df_non_spam.loc[valid_text_mask, 'commentCategory'] = classified_categories\n\n# Display a preview of the results\nprint(df_non_spam[['spam', 'textCleaned', 'commentCategory']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:27:41.624150Z","iopub.execute_input":"2025-09-09T15:27:41.624765Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n\ncomments_to_analyze = df_non_spam['textCleaned'].tolist()\n\nif comments_to_analyze:\n    print(f\"Analyzing sentiment for {len(comments_to_analyze)} comments...\")\n    results = sentiment_pipe(comments_to_analyze, batch_size=64, truncation=True)\n\n    df_non_spam['sentiment_label'] = [result['label'] for result in results]\n    df_non_spam['sentiment_score'] = [result['score'] for result in results]\n\nelse:\n    print(\"No comments to analyze.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T13:38:34.819111Z","iopub.execute_input":"2025-09-09T13:38:34.819760Z","iopub.status.idle":"2025-09-09T13:49:14.874056Z","shell.execute_reply.started":"2025-09-09T13:38:34.819736Z","shell.execute_reply":"2025-09-09T13:49:14.873131Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6226fd09b50b4f13a97290ee429c9356"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e8d2da0efe84dee8f963a7f47a18f5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8c6e99a5e74405aa0ff494b33cb57e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"649d82eb43554831badb6d5333add85a"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Analyzing sentiment for 138956 comments...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/664397523.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_non_spam['sentiment_label'] = [result['label'] for result in results]\n/tmp/ipykernel_36/664397523.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_non_spam['sentiment_score'] = [result['score'] for result in results]\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"df_non_spam.to_csv('final_comments.csv', index=False, encoding='utf-8-sig')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}